- import_playbook: healthy_raftserver_cluster_type1.yml
- name: "partition_split_and_rejoin"
  hosts: localhost
  connection: local
  vars:
     recipe_name: "partition_split_and_rejoin"
     parent: "healthy_raftserver_cluster_type1"
     requirement: "pumicedb"
     num_writes: 100
  tasks:
  - block:
    - name: "Check if parent recipe failed"
      debug: msg="Check if parent recipe {{ parent }} failed"
      failed_when: terminate_recipe == true

    #All peers in the cluster should be up and running.
    - name: "{{ recipe_name }}: Verifying recipe compatibility requirements."
      include_role:
         name: common
         tasks_from: recipe_compatibility_requirement

    - name: "{{ recipe_name }}: Get the list of all running peer UUIDs"
      include_role:
         name: common
         tasks_from: get_server_uuid_info

    #Recipe Setup: If the cluster is not booted, the recipe should abort.
    - name: "{{ recipe_name }}: Verify if the cluster is not booted, the recipe should abort."
      include_role:
         name: common
         tasks_from: verify_all_peers_up_and_running

    - name: "{{ recipe_name }}: Get Leader UUID and corresponding UUIDs for follower peers"
      include_role:
         name: common
         tasks_from: get_follower_stats

    #Recipe Setup: Ensure followers aliveness.
    - name: "{{ recipe_name }}: Validate followers are alive."
      include_role:
         name: common
         tasks_from: validate_followers_aliveness
      vars:
         ValLeaderUUID: "{{ LeaderUUID['/0/leader-uuid'] }}"

    #Start 2 client instances.
    - name: "{{ recipe_name }}: Get multiple client uuids."
      include_role:
        name: common
        tasks_from: get_multiple_client_uuids
      loop: "{{ range(0, 2) | list }}" #loop for no. of client uuids required.

    - name: "{{ recipe_name }}: Generate App UUIDs."
      shell: "/usr/bin/uuid"
      register: partition_app_uuid

    - name: "{{ recipe_name }}: Start the client."
      include_role:
         name: common
         tasks_from: start_client
      vars:
        ClientUUID: "{{ ClientUUIDS[item] }}"
      loop: "{{ range(0, ClientUUIDS | length) | list }}"

    #Increase election timeout to 500ms.
    - name: "{{ recipe_name }}: Set election timeout to 500 ms for all servers."
      vars:
        stage: "set_elec_timeout500ms"
        wait_for_ofile: False
        cmd: "election-timeout-ms@500"
        where: "/raft_net_info/election-timeout-ms"
        key:
         - "/raft_net_info/election-timeout-ms"
      debug:
        msg:
          - "{{ lookup('niova_ctlrequest', 'apply_cmd', NRunningPeers, cmd, where, wantlist=True) }}"
      no_log: true

    #Start writes from both two clients. 100writes each.
    - name: "{{ recipe_name }}: Perform the write {{ num_writes }} times."
      vars:
        stage: "num_writes"
        cmd: "input@{{ partition_app_uuid.stdout }}:0:0:0:0.write:0.{{ num_writes }}"
        where: "/pumice_db_test_client/input"
        write_cmd: "{{ lookup('niova_ctlrequest', 'apply_cmd', ClientUUIDS, cmd, where, wantlist=True) }}"
      debug:
        msg: "{{ write_cmd }}"
      no_log: True

    #Prepare list of two sets of the peers in the cluster.
    - name: "Get the peer uuids for creating partition with enough number of peers to form the quorum."
      vars:
        get_quorum: "{{ FollowerUUIDs | length | int / 2 + 1 | int }}"
      set_fact:
        Quorum_for_Partition: "{{ get_quorum | int }}"

    #Bigger Cluster(Partition1).
    - name: "Set Partition1(Bigger Cluster)."
      vars:
        Partition1: []
      set_fact:
        Partition1: "{{ Partition1 + [FollowerUUIDs[item]] }}"
      loop: "{{ range(0,  Quorum_for_Partition | int) | list }}"

    - debug:
        msg: "Bigger Cluster: {{ Partition1 }}"

    #Smaller Cluster(Partition2).The existing leader should be part of paritition2.
    - name: "Set Partition2(Smaller Cluster)."
      vars:
        remaining_uuids: "{{ FollowerUUIDs | difference(Partition1) }}"
      set_fact:
        Partition2: "{{ [] + [LeaderUUID['/0/leader-uuid']] + remaining_uuids }}"

    - debug:
        msg: "Smaller Cluster: {{ Partition2 }}"

    #Disabling Msg(Recv) from Partition1 to Partition2.
    - name: "{{ recipe_name }}: Disable Msg(Recv) from Partition1 to Partition2."
      vars:
        stage: "disable_candidate_mode"
        cmd: "net_recv_enabled@false"
        where: "/ctl_svc_nodes/uuid@{{ Partition1[item] }}"
        disable_mode: "{{ lookup('niova_ctlrequest', 'apply_cmd', Partition2, cmd, where, wantlist=True) }}"
      debug:
        msg: "{{ disable_mode }}"
      loop: "{{ range(0, Partition1 | length) | list }}"
      no_log: True

    #Disabling Msg(Recv) from Partition2 to Partition1.
    - name: "{{ recipe_name }}: Disable Msg(Recv) from Partition2 to Partition1."
      vars:
        stage: "disable_candidate_mode"
        cmd: "net_recv_enabled@false"
        where: "/ctl_svc_nodes/uuid@{{ Partition2[item] }}"
        disable_mode: "{{ lookup('niova_ctlrequest', 'apply_cmd', Partition1, cmd, where, wantlist=True) }}"
      debug:
        msg: "{{ disable_mode }}"
      loop: "{{ range(0, Partition2 | length) | list }}"
      no_log: True

    - name: "{{ recipe_name }}: Get the leader from partition2."
      vars:
         stage: "part2_leader"
         leader_key:
            - "/raft_root_entry/0/leader-uuid"
         verify_partition2: "{{ lookup('niova_ctlrequest', 'lookup', Partition2, leader_key, wantlist=True) }}"
      debug:
        msg: "Getting orignal values {{ verify_partition2 }}."
      no_log: True
      with_items:
          - "{{ verify_partition2 }}"
      register: part2_value

    #Check the partition created in the cluster.Check the leader UUIDs on both two sets.
    - name: "{{ recipe_name }}: Verify the smaller cluster's leader is same as original leader."
      vars:
        part2_stage: "{{ part2_value['results'][0]['item'] }}"
      debug:
        msg:
          - "Verifying new leader election on partition2."
      no_log: True
      failed_when: part2_stage['/0/leader-uuid'] != LeaderUUID['/0/leader-uuid']

    #On Partition1, check for a new leader and successful leader election.
    - name: "{{ recipe_name }}: Wait until the new leader elected in bigger cluster(Partition1)."
      vars:
        stage: "wait_leader_to_elect"
        raft_key: "/raft_root_entry/0/leader-uuid"
      debug:
        msg: "Waiting for leader to elect"
      until: lookup('niova_ctlrequest', 'lookup', Partition1[item], raft_key, wantlist=True) | dict2items | map(attribute='value') | list | first != LeaderUUID['/0/leader-uuid']
      retries: 60
      delay: 1
      loop: "{{ range(0, Partition1 | length) | list }}"

    #On Partition 2, check for ‘may-be-depose’
    - name: "{{ recipe_name }}: Wait until the leader notices 'may-be-depose'."
      vars:
        stage: "wait_leader_may_be_depose"
        part2_leader: "{{ part2_value['results'][0]['item'] }}"
        raft_key: "/raft_root_entry/0/state"
      debug:
        msg: "Waiting for leader notices 'may-be-depose'"
      until: lookup('niova_ctlrequest', 'lookup', part2_leader['/0/leader-uuid'], raft_key, wantlist=True) | dict2items | map(attribute='value') | list | first == "may-be-depose"
      retries: 60
      delay: 1

    - name: "{{ recipe_name }}: Gather the stats for Partition1(Bigger Cluster)."
      vars:
         stage: "verify_partition1"
         raft_keys:
              - "/raft_root_entry/0/commit-idx"
              - "/raft_root_entry/0/last-applied"
              - "/raft_root_entry/0/last-applied-cumulative-crc"
              - "/raft_root_entry/0/sync-entry-crc"
              - "/raft_root_entry/0/term"
              - "/raft_root_entry/0/sync-entry-term"
         peer_values: "{{ lookup('niova_ctlrequest', 'lookup', Partition1, raft_keys, wantlist=True) }}"
      debug:
        msg: "Getting stats for all running peers."
      no_log: true
      with_items:
        - "{{ peer_values }}"
      register: running_peers_info

    - name: "{{ recipe_name }}: Compare the values for successful leader election of bigger cluster(Partition1)."
      vars:
        peer_info: "{{ running_peers_info['results'][item]['item'] }}"
      debug:
        msg:
        - "Validate the raft values for bigger cluster."
      failed_when: >
        (peer_info["/0/commit-idx"] < 0) or
        (peer_info["/0/last-applied"] < 0) or
        (peer_info["/0/last-applied-cumulative-crc"] != peer_info["/0/sync-entry-crc"]) or
        (peer_info["/0/term"] != peer_info["/0/sync-entry-term"])
      loop: "{{ range(0, Partition1 | length) | list }}"

    #Enable msg recv on peers from two partitions.
    - name: "{{ recipe_name }}: Enable Msg(Recv) from Partition1 to Partition2."
      vars:
        stage: "disable_candidate_mode"
        cmd: "net_recv_enabled@true"
        where: "/ctl_svc_nodes/uuid@{{ Partition1[item] }}"
        disable_mode: "{{ lookup('niova_ctlrequest', 'apply_cmd', Partition2, cmd, where, wantlist=True) }}"
      debug:
        msg: "{{ disable_mode }}"
      loop: "{{ range(0, Partition1 | length) | list }}"
      no_log: True

    - name: "{{ recipe_name }}: Enable Msg(Recv) from Partition2 to Partition1."
      vars:
        stage: "disable_candidate_mode"
        cmd: "net_recv_enabled@true"
        where: "/ctl_svc_nodes/uuid@{{ Partition2[item] }}"
        disable_mode: "{{ lookup('niova_ctlrequest', 'apply_cmd', Partition1, cmd, where, wantlist=True) }}"
      debug:
        msg: "{{ disable_mode }}"
      loop: "{{ range(0, Partition2 | length) | list }}"
      no_log: True

    #Loop for Partition1(bigger cluster) to get merged in Partition2(smaller cluster).
    - name: "{{ recipe_name }}: Get the leader(which elected in bigger cluster) from all peers."
      vars:
         stage: "get_leader"
         leader_key:
            - "/raft_root_entry/0/leader-uuid"
         verify_leader: "{{ lookup('niova_ctlrequest', 'lookup', Partition1, leader_key, wantlist=True) }}"
      debug:
        msg: "Getting orignal values {{ verify_leader }}."
      no_log: True
      with_items:
          - "{{ verify_leader }}"
      register: leader_value

    - name: "{{ recipe_name }}: Wait until all the peers report same leader UUID which is elected in bigger cluster."
      vars:
        stage: "verify_leader_on_all"
        part1_leader: "{{ leader_value['results'][0]['item']['/0/leader-uuid'] }}"
        raft_key: "/raft_root_entry/0/leader-uuid"
      debug:
        msg: "Verifying leader-uuid on all peers."
      until: lookup('niova_ctlrequest', 'lookup', NRunningPeers[item], raft_key, wantlist=True) | dict2items | map(attribute='value') | list | first == (part1_leader)
      retries: 60
      delay: 1
      loop: "{{ range(0, NRunningPeers | length) | list }}"

    #Perform writes from all clients. 100writes from each client.
    - name: "{{ recipe_name }}: Perform the write {{ num_writes }} times on both clients."
      vars:
        stage: "num_writes100"
        cmd: "input@{{ partition_app_uuid.stdout }}:0:0:0:0.write:0.{{ num_writes }}"
        where: "/pumice_db_test_client/input"
        again_write_cmd: "{{ lookup('niova_ctlrequest', 'apply_cmd', ClientUUIDS, cmd, where, wantlist=True) }}"
      debug:
        msg: "{{ again_write_cmd }}"
      no_log: True

    - name: "{{ recipe_name }}: Wait until all write completes from both client."
      vars:
        stage: "wait_to_client_write"
        raft_key: "/pumice_db_test_client/pmdb-test-apps/0/pmdb-seqno"
      debug:
        msg: "Waiting for client to finish writing"
      until: lookup('niova_ctlrequest', 'lookup', ClientUUIDS[item], raft_key, wantlist=True) | dict2items | map(attribute='value') | list | first == (num_writes | int - 1)
      retries: 5
      delay: 1
      loop: "{{ range(0, ClientUUIDS | length) | list }}"

