- import_playbook: healthy_raftserver_cluster_type1.yml
- name: "partition_split_and_rejoin"
  hosts: localhost
  connection: local
  vars:
     recipe_name: "partition_split_and_rejoin"
     parent: "healthy_raftserver_cluster_type1"
     requirement: "pumicedb"
     number_of_app_uuids: 1
     server_keys:
              - "/raft_root_entry/0/leader-uuid"
              - "/raft_root_entry/0/commit-idx"
              - "/raft_root_entry/0/last-applied"
              - "/raft_root_entry/0/last-applied-cumulative-crc"
              - "/raft_root_entry/0/sync-entry-crc"
              - "/raft_root_entry/0/term"
              - "/raft_root_entry/0/sync-entry-term"
  tasks:
  - block:
    - name: "Check if parent recipe failed"
      debug: msg="Check if parent recipe {{ parent }} failed"
      failed_when: terminate_recipe == true

    #All peers in the cluster should be up and running.
    - name: "{{ recipe_name }}: Verifying recipe compatibility requirements."
      include_role:
         name: common
         tasks_from: recipe_compatibility_requirement

    - name: "{{ recipe_name }}: Get the list of all running peer UUIDs"
      include_role:
         name: common
         tasks_from: get_server_uuid_info

    #Recipe Setup: If the cluster is not booted, the recipe should abort.
    - name: "{{ recipe_name }}: Verify if the cluster is not booted, the recipe should abort."
      include_role:
         name: common
         tasks_from: verify_all_peers_up_and_running

    - name: "{{ recipe_name }}: Get Leader UUID and corresponding UUIDs for follower peers"
      include_role:
         name: common
         tasks_from: get_follower_stats

    #Recipe Setup: Ensure followers aliveness.
    - name: "{{ recipe_name }}: Validate followers are alive."
      include_role:
         name: common
         tasks_from: validate_followers_aliveness
      vars:
         ValLeaderUUID: "{{ LeaderUUID['/0/leader-uuid'] }}"

    - name: "{{ recipe_name}}: Get the server information for all running servers."
      vars:
        stage: "server_verify"
        peer_values_all: "{{ lookup('niova_ctlrequest', 'lookup', NRunningPeers, server_keys, wantlist=True) }}"
      debug:
        msg: "Get the values for all running servers {{ peer_values_all }}"
      no_log: true
      with_items:
        - "{{ peer_values_all }}"
      register: running_srv_vals

    #start multiple clients using common task.
    - name: "{{ recipe_name }}: start multiple clients."
      include_role:
        name: common
        tasks_from: start_multiple_clients
      vars:
        number_of_clients: 2

    #Create set of required number of app_uuids.
    - name: "Get set of required number of app_uuids."
      include_role:
        name: common
        tasks_from: create_app_uuid_set
      vars:
        number_of_apps: "{{ number_of_app_uuids }}"

    #Increase election timeout to 500ms.
    - name: "{{ recipe_name }}: Set election timeout to 500 ms for all servers."
      vars:
        stage: "set_elec_timeout500ms"
        wait_for_ofile: False
        cmd: "election-timeout-ms@500"
        where: "/raft_net_info/election-timeout-ms"
        key:
         - "/raft_net_info/election-timeout-ms"
      debug:
        msg:
          - "{{ lookup('niova_ctlrequest', 'apply_cmd', NRunningPeers, cmd, where, wantlist=True) }}"
      no_log: true

    - name: "Perform write operations for multiple clients."
      include_role:
         name: common
         tasks_from: perform_writes
      vars:
        pmdb_apps: "{{ pmdb_app_uuids }}"
        ClientUUID: "{{ ClientUUIDS[idx] }}"
        constant_number_of_writes: 300
      loop: "{{ range(0, ClientUUIDS | length) |list }}"
      loop_control:
              loop_var: idx

    #Prepare list of two sets of the peers in the cluster.
    - name: "Select peers for Large (quorum-capable) partition."
      vars:
        get_quorum: "{{ FollowerUUIDs | length | int / 2 + 1 | int }}"
      set_fact:
        Quorum_for_Partition: "{{ get_quorum | int }}"

    #Bigger Cluster.
    - name: "Set Bigger Cluster."
      vars:
        bigger_cluster: []
      set_fact:
        bigger_cluster: "{{ bigger_cluster + [FollowerUUIDs[item]] }}"
      loop: "{{ range(0,  Quorum_for_Partition | int) | list }}"

    - debug:
        msg: "Bigger Cluster: {{ bigger_cluster }}"

    #Smaller Cluster.The existing leader should be part of Smaller Cluster.
    - name: "Set Smaller Cluster."
      vars:
        remaining_uuids: "{{ FollowerUUIDs | difference(bigger_cluster) }}"
      set_fact:
        smaller_cluster: "{{ [] + [LeaderUUID['/0/leader-uuid']] + remaining_uuids }}"

    - debug:
        msg: "Smaller Cluster: {{ smaller_cluster }}"

    #Disabling Msg(Recv) from bigger_cluster to smaller_cluster.
    - name: "{{ recipe_name }}: Disable Msg(Recv) from bigger_cluster to smaller_cluster."
      vars:
        stage: "disable_candidate_mode"
        cmd: "net_recv_enabled@false"
        where: "/ctl_svc_nodes/uuid@{{ bigger_cluster[item] }}"
        disable_mode: "{{ lookup('niova_ctlrequest', 'apply_cmd', smaller_cluster, cmd, where, wantlist=True) }}"
      debug:
        msg: "{{ disable_mode }}"
      loop: "{{ range(0, bigger_cluster | length) | list }}"
      no_log: True

    #Disabling Msg(Recv) from smaller_cluster to bigger_cluster.
    - name: "{{ recipe_name }}: Disable Msg(Recv) from smaller_cluster to bigger_cluster."
      vars:
        stage: "disable_candidate_mode"
        cmd: "net_recv_enabled@false"
        where: "/ctl_svc_nodes/uuid@{{ smaller_cluster[item] }}"
        disable_mode: "{{ lookup('niova_ctlrequest', 'apply_cmd', bigger_cluster, cmd, where, wantlist=True) }}"
      debug:
        msg: "{{ disable_mode }}"
      loop: "{{ range(0, smaller_cluster | length) | list }}"
      no_log: True

    #On bigger_cluster, check for a new leader and successful leader election.
    - name: "{{ recipe_name }}: Wait until the new leader elected in bigger cluster."
      vars:
        stage: "wait_leader_to_elect"
        raft_key:
              - "/raft_root_entry/0/leader-uuid"
      debug:
        msg: "Waiting for leader to elect"
      until: lookup('niova_ctlrequest', 'lookup', bigger_cluster[item], raft_key, wantlist=True) | dict2items | map(attribute='value') | list | first != LeaderUUID['/0/leader-uuid']
      retries: 60
      delay: 1
      loop: "{{ range(0, bigger_cluster | length) | list }}"

    - name: "{{ recipe_name }}: Get the leader from smaller cluster."
      vars:
         stage: "small_cluster_leader"
         leader_key: "/raft_root_entry/0/leader-uuid"
         verify_smaller_cluster: "{{ lookup('niova_ctlrequest', 'lookup', smaller_cluster, leader_key, wantlist=True) }}"
      debug:
        msg: "Getting orignal values {{ verify_smaller_cluster }}"
      no_log: True
      with_items:
          - "{{ verify_smaller_cluster }}"
      register: part2_value

    #Check the partition created in the cluster.Check the leader UUIDs on both two sets.
    - name: "{{ recipe_name }}: Verify the smaller cluster's leader is same as original leader."
      vars:
        part2_stage: "{{ part2_value['results'][0]['item'] }}"
      debug:
        msg:
          - "Verifying new leader election on partition2."
      no_log: True
      failed_when: part2_stage['/0/leader-uuid'] != LeaderUUID['/0/leader-uuid']

    #On Bigger Cluster, check for ‘deny-may-be-depose’
    - name: "{{ recipe_name }}: Wait until the leader notices 'deny-may-be-depose'."
      vars:
        stage: "wait_leader_may_be_depose"
        part2_leader: "{{ part2_value['results'][0]['item'] }}"
        raft_key: "/raft_root_entry/0/client-requests"
      debug:
        msg: "Waiting for leader notices 'deny-may-be-deposed'"
      until: lookup('niova_ctlrequest', 'lookup', part2_leader['/0/leader-uuid'], raft_key, wantlist=True) | dict2items | map(attribute='value') | list | first == "deny-may-be-deposed"
      retries: 60
      delay: 1

    #Enable msg recv on peers from two partitions(i.e.Remove partition)
    - name: "{{ recipe_name }}: Enable Msg(Recv) from bigger_cluster to smaller_cluster."
      vars:
        stage: "disable_candidate_mode"
        cmd: "net_recv_enabled@true"
        where: "/ctl_svc_nodes/uuid@{{ bigger_cluster[item] }}"
        disable_mode: "{{ lookup('niova_ctlrequest', 'apply_cmd', smaller_cluster, cmd, where, wantlist=True) }}"
      debug:
        msg: "{{ disable_mode }}"
      loop: "{{ range(0, bigger_cluster | length) | list }}"
      no_log: True

    - name: "{{ recipe_name }}: Enable Msg(Recv) from smaller_cluster to bigger_cluster."
      vars:
        stage: "disable_candidate_mode"
        cmd: "net_recv_enabled@true"
        where: "/ctl_svc_nodes/uuid@{{ smaller_cluster[item] }}"
        disable_mode: "{{ lookup('niova_ctlrequest', 'apply_cmd', bigger_cluster, cmd, where, wantlist=True) }}"
      debug:
        msg: "{{ disable_mode }}"
      loop: "{{ range(0, smaller_cluster | length) | list }}"
      no_log: True

    #Loop for bigger_cluster(bigger cluster) to get merged in smaller_cluster(smaller cluster).
    - name: "{{ recipe_name }}: Get the leader(which elected in bigger cluster) from all peers."
      vars:
         stage: "get_leader"
         leader_key:
            - "/raft_root_entry/0/leader-uuid"
         verify_leader: "{{ lookup('niova_ctlrequest', 'lookup', bigger_cluster, leader_key, wantlist=True) }}"
      debug:
        msg: "Getting orignal values {{ verify_leader }}."
      no_log: True
      with_items:
          - "{{ verify_leader }}"
      register: leader_value

    - name: "{{ recipe_name }}: Wait until all the peers report same leader UUID which is elected in bigger cluster."
      vars:
        stage: "verify_leader_on_all"
        part1_leader: "{{ leader_value['results'][0]['item']['/0/leader-uuid'] }}"
        raft_key: "/raft_root_entry/0/leader-uuid"
      debug:
        msg: "Verifying leader-uuid on all peers."
      until: lookup('niova_ctlrequest', 'lookup', NRunningPeers[item], raft_key, wantlist=True) | dict2items | map(attribute='value') | list | first == (part1_leader)
      retries: 60
      delay: 1
      loop: "{{ range(0, NRunningPeers | length) | list }}"

    - name: "{{ recipe_name}}: Get the server information for all running servers."
      vars:
        stage: "all_peers_verify"
        server_values: "{{ lookup('niova_ctlrequest', 'lookup', NRunningPeers, server_keys, wantlist=True) }}"
      debug:
        msg: "Get the values for all running servers {{ server_values }}"
      no_log: true
      with_items:
        - "{{ server_values }}"
      register: all_peers_vals

    - name: "{{ recipe_name }}: Compare the values for successful leader election of Bigger Cluster."
      vars:
        orig_values: "{{ running_srv_vals['results'][item]['item'] }}"
        peer_info: "{{ all_peers_vals['results'][item]['item'] }}"
      debug:
        msg:
        - "Validate the raft values for bigger cluster."
      failed_when: >
        (peer_info["/0/commit-idx"] <= orig_values["/0/commit-idx"]) or
        (peer_info["/0/last-applied"] <= orig_values["/0/last-applied"]) or
        (peer_info["/0/term"] <= orig_values["/0/term"])
      loop: "{{ range(0, NRunningPeers | length) | list }}"

    - name: "{{ recipe_name}}: Verify that term, last-applied-cumulative-crc and sync-entry-crc are same on all peers"
      vars:
        curr_itr_vals: "{{ all_peers_vals['results'][item]['item'] }}"
        next_itr_vals: "{{ all_peers_vals['results'][item + 1]['item'] }}"
      debug:
        msg: "Verifying that term, last-applied-cumulative-crc and sync-entry-crc are same on all peers"
      failed_when: >
         (curr_itr_vals['/0/last-applied-cumulative-crc'] != next_itr_vals['/0/last-applied-cumulative-crc']) or
         (curr_itr_vals['/0/sync-entry-crc'] != next_itr_vals['/0/sync-entry-crc'])
      loop: "{{ range(0, NRunningPeers | length - 1)| list }}"

    - name: "Perform write operations for multiple clients."
      include_role:
         name: common
         tasks_from: perform_writes
      vars:
        pmdb_apps: "{{ pmdb_app_uuids }}"
        ClientUUID: "{{ ClientUUIDS[idx] }}"
        constant_number_of_writes: 300
      loop: "{{ range(0, ClientUUIDS | length) |list }}"
      loop_control:
              loop_var: idx

    #wait for write operations to complete.
    - name: "{{ recipe_name }}: wait for write completion."
      include_role:
        name: common
        tasks_from: wait_for_write_completion
      vars:
        Client_UUID: "{{ ClientUUIDS[cl_idx] }}"
      loop: "{{  range(0, ClientUUIDS | length) | list }}"
      loop_control:
              loop_var: cl_idx

